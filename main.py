#A python bot sitting atop PyWikibot and Doxygen to automatically
#add doxygen docs to a wiki for your documentation pleasure

#The main goal of this is to take the power and placement of doxygen docs
#and combine it with the flexibility and remoteness of a wiki.

#TODO:
#fix all the TODOS in the code
#Clone doxygen's easy navigation with some templates for mediawiki? maybe
#Special styles for the doxygen wikipedia docs
#Make sure to include links and logos to doxygen so we properly attribute them
#Interactive mode
#Cleanup mode (so that we can delete all the autogenerated docs if necessary, like switching from category to namespace version)
#Option for deleting old docs right away or leaving them around for one "update"(as is currently the default)

import re
import os
import subprocess
from html.parser import HTMLParser

from PyWikibot.scripts import login, upload
from PyWikibot.pywikibot import pagegenerators

#A HTML Parser that simply keeps track of our current position with a stack
#TODO: Make or use more powerful HTML Parser. The current functionality
#  the current needs but is definitely a bit unforgiving when replacing tags
class HTMLParserStackException(Exception):
    pass
class HTMLTagIdentifier(object):
    def __init__(self, tag, attrs):
        self.tag = tag
        #Convert tuple list to dictionary
        self.attrs = {}
        for item in attrs:
            self.attrs[item[0]] = item[1]

    def getIdentStr(self):
        #Build a full tag identifier like CSS
        #Only class attribute currently needs support
        fullTag = self.tag
        if "class" in self.attrs:
            fullTag += "." + self.attrs["class"]
        
        return fullTag
class HTMLParserStack(HTMLParser):        
    def __init__(self):
        super().__init__()
        self.tagStack = []
    
    def handle_starttag(self, tag, attrs):
        #Keep track of tags on the stack
        newTag = HTMLTagIdentifier(tag, attrs)
        self.tagStack.append(newTag)
    
    def handle_endtag(self, tag):
        #Pop off a tag and compare it with the stack
        pastTag = self.tagStack.pop()
        if pastTag.tag != tag:
            #Popped tag does not match the current stack, that's not right...
            #This could be a sign of malformed HTML or something with a <dd> tag (look that up #TODO:)
            #Keep popping until we find the first correct tag, if it was just an extra tag or a few on the stack we're good
            #If not it'll fuck up the stack so when we pop later we'll eventually end up popping the entire stack looking for the next tag
            #  and that's where we catch the real malformed HTML error (when a tags are closed in the wrong order or other things)
            print("WARNING: Tag popped doesn't match stack")
            
            #Keep popping until we get the right tag
            #Keep original for error reporting
            origTagStack = self.tagStack[:]
            origTagStack.append(pastTag)
            while len(self.tagStack) > 0:
                pastTag = self.tagStack.pop()
                if pastTag.tag == tag:
                    break
                
            if(len(self.tagStack) <= 0):
                #We never found a matching tag to pop, uh oh
                #Raise an error
                str=[]
                for tag in origTagStack:
                    str.append(tag.getIdentStr())
                
                raise HTMLParserStackException("Invalid HTML encountered, improper tag closure at " + " > ".join(str))
    
    #Test the tag stack
    def testStack(self, test):
        if len(self.tagStack) != len(test):
            return False
        
        for i in range(len(self.tagStack)):
            if self.tagStack[i].getIdentStr() != test[i]:
                return False
        return True

#A HTMLParser that extracts portions of a doxygen HTML file
#The data attribute is of the most use to us
#It comes containing a dictionary of strings to HTML data
#These strings are:
# + title: The title of the Doxygen file
# + contents: The body of the Doxygen file
class DoxygenHTMLExtractor(HTMLParserStack):
    def __init__(self):
        super().__init__()
        
        self.data = {}
        self.captureDataAs = None
        self.captureDataUntil = None
        
    def handle_starttag(self, tag, attrs):
        super().handle_starttag(tag, attrs)
        self.startTag(tag, attrs)
    
    def handle_startendtag(self, tag, attrs):
        self.startTag(tag, attrs)
    
    def startTag(self, tag, attrs):
        #Recording data
        if self.captureDataAs != None:
            self.data[self.captureDataAs] += self.get_starttag_text()
            return
            
        #Look for specific tags to capture data from
        titleStack = ["html", "body", "div.header", "div.headertitle", "div.title"]
        contentsStack = ["html", "body", "div.contents"]
        if self.testStack(titleStack):
            self.captureDataAs = "title"
            self.captureDataUntil = titleStack
            self.data[self.captureDataAs] = ""
        elif self.testStack(contentsStack):
            self.captureDataAs = "contents"
            self.captureDataUntil = contentsStack
            self.data[self.captureDataAs] = ""
    
    def handle_endtag(self, tag):
        #If we're back at the same tagStack, stop capturing data
        if self.captureDataAs != None:
            if self.testStack(self.captureDataUntil):
                self.captureDataAs = None
                self.captureDataUntil = None
        
        #Recording data
        if self.captureDataAs != None:        
            self.data[self.captureDataAs] += "</" + tag + ">"
        
        super().handle_endtag(tag)
    
    def handle_data(self, data):
        #Recording data
        if self.captureDataAs != None:
            self.data[self.captureDataAs] += data
            return

#A HTMLParser that replaces all HTML with wiki compatible HTML
#Two attributes are of most use to us
#data contains all of the HTML originally fed to it with modifications for the wiki
#imgs contains all the image files that were identified that need to be uploaded to the wiki
class DoxygenHTMLReplacer(HTMLParser):
    def __init__(self, wikiPages):
        super().__init__()
        
        self.wikiPages = wikiPages #Allows us to change links with other wiki pages
        self.data = ""
        self.imgs = []
        
        #TODO: Convert lastTag over to either HTMLParserStack (add more testing functionality) or
        #  get a more powerful HTML parser
        self.lastTag = None
    def handle_starttag(self, tag, attrs):
        self.lastTag = tag
        self.startTag(tag, attrs)
    
    def handle_startendtag(self, tag, attrs):
        self.startTag(tag, attrs)
    
    def startTag(self, tag, attrs):
        #Output of doxygen
        #http://www.stack.nl/~dimitri/doxygen/manual/htmlcmds.html

        #Accepted by mediawiki
        #http://meta.wikimedia.org/wiki/Help:HTML_in_wikitext

        #Output from doxygen and not supported by mediawiki
        #We must convert these
        #<a href="...">
        #<a name="...">
        #<img src="..." ...>
        newTag = HTMLTagIdentifier(tag, attrs)
        
        if newTag.tag == "a":
            #TODO: Somehow add the data of a into the alternative link text section
            self.data += "[["
            if "href" in newTag.attrs:
                href = newTag.attrs["href"]
                hashPos = href.rfind("#")
                fragment = ""
                if hashPos != -1:
                    fragment = href[hashPos:]
                    link = href[:hashPos]
                else:
                    link = href
                
                foundMatch = False
                for page in self.wikiPages:
                    if link == page.filename:
                        foundMatch = True
                        link = page.title
                        break
                
                if not foundMatch:
                    print("WARNING: Couldn't find suitable match for link " + link)
                else:
                    self.data += "[[" + link + fragment + "]]"
            if "name" in newTag.attrs:
                self.data += "<span id=\"" + newTag.attrs["name"] + "\"></span>" #Named anchors in MediaWiki just use the id
            
        elif newTag.tag == "img":
            #TODO: Pic how we're going to style this on the wiki, just uses default
            self.data += "[[File:" + newTag.attrs["src"] + "]]"
            #Make a note of the image
            self.imgs.append(newTag.attrs["src"])
        
        #Not a tag we need to convert
        else:
            self.data += self.get_starttag_text()
    
    def handle_endtag(self, tag):
        #Not a tag we need to convert
        #MW links and imgs are handled in the other handler, no output needed
        if tag != "a" and tag != "img":
            self.data += "</" + tag + ">"
        
    def handle_data(self, data):
        if self.lastTag != "a" or self.lastTag != "img":
            self.data += data

class DoxygenWikiPage(object):
    def __init__(self):
        #About the file this page came from (every file is a page)
        self.filepath = None
        self.filename = None
        
        #About the page itself
        self.type = None
        self.title = None
        self.contents = None
        
            
def main():
    #Options:
    options = {}
    
    #DoxygenMediawikibot
    #options["interactive"] = False
    #options["printLevel"] = 0
    #options[""] = whatever
    
    
    #Doxygen related path info
    options["doxygen_binaryPath"] = "C:/Program Files/doxygen/bin"
    options["doxygen_configPath"] = "C:/Users/PeterF/Desktop/DoxygenWikibot/DoxyfileTest" 
    
    #Parameters we must force to generate proper, small, output
    options["doxygen_paramsForce"] = {
    #Output file format and location
    #Critical
    "OUTPUT_DIRECTORY"       : "\"./tmp\"",
    "GENERATE_HTML"          : "YES",
    "HTML_OUTPUT"            : "html",
    "HTML_FILE_EXTENSION"    : ".html",
    
    #Disabling specific HTML sections
    #Possibly critical, makes HTML easier to work with
    "DISABLE_INDEX"          : "YES",
    "SEARCHENGINE"           : "NO",
    
    #Turn off other generation
    #Not critical but wanted
    #Extra HTML
    "GENERATE_DOCSET"        : "NO",
    "GENERATE_HTMLHELP"      : "NO",
    "GENERATE_QHP"           : "NO",
    "GENERATE_ECLIPSEHELP"   : "NO",
    "GENERATE_TREEVIEW"      : "NO",
    
    #Other generations
    "GENERATE_LATEX"         : "NO",
    "GENERATE_RTF"           : "NO",
    "GENERATE_XML"           : "NO",
    "GENERATE_DOCBOOK"       : "NO",
    "GENERATE_AUTOGEN_DEF"   : "NO",
    "GENERATE_PERLMOD"       : "NO"
    }
    
    #Parameters we warn about but do not enforce
    options["doxygen_paramsWarn"] = {
    "CASE_SENSE_NAMES"       : "NO" #MediaWiki doesn't support case sensitivity in title names
    }
    
    #Doxygen generates all it's files with prefixes by type
    #TODO: This is not an exhaustive list, some configuration patterns have not been tested
    #Files, prefix "_"
    #Interfaces, prefix "interface_"
    #Namespaces, prefix "namespace_"
    #Classes, prefix "class_"
    options["doxygen_filePrefixes"] = {
        "_" : "FILE",
        "namespace_" : "NAMESPACE",
        "class_" : "CLASS",
        "interface_" : "INTERFACE"
    }
    
    #Other files we want (useful and don't provide redundancies to MediaWiki functionality)
    #Class hierarchy, hierarchy.html
    options["doxygen_otherFiles"] = [
        "hierarchy"
    ]
    
    #MediaWiki stuff
    options["mediaWiki_separationName"] = "DoxygenDocs" #The category or namespace name to use for the doxygen docs, should not be empty
    options["mediaWiki_useCustomNamespace"] = False #TODO: (not imeplemented) Whether to use a custom namespace or just a category for all of the doxygen documents
    
    #All pages get another page generated with just the content along with a transcription
    #The links are properly setup to then always point to the transcription pages
    #This allows us to put all the autogenerated data in a locked down part of the wiki
    #It also allows quick, dirty, and easy updating of the doxygen docs while allowing users to just configure their
    #own text on the transcription pages
    options["mediaWiki_setupTranscriptions"] = True
    options["mediaWiki_transcriptionPrefix"] = "" #Prefix of transcription, can be empty
    options["mediaWiki_transcriptionCategory"] = "CodingDocs" #Category to add all these pages to, can be no category (to turn off)

    #( 1 ) Generate the doxygen docs
    #Try the config file
    with open(options["doxygen_configPath"]) as fp:
        configLines = fp.readlines()
        fp.seek(0)
        config = fp.read()
        
        #Read each line for params to warn about
        warnParams = options["doxygen_paramsWarn"]
        for line in configLines:
            #Comments
            if line[0] == "#":
                continue
            
            match = re.match('\s*(\S+)\s*=\s+(\S*)', line)
            if match:
                k, v = match.group(0,1)
                
                #Warn about specific parameters
                for warn in warnParams.keys():
                    if k == warn and v != warnParams[warn]:
                        print("WARNING: Parameter " + warn + " is not set to " + warnParams[warn])
                
        #Append the force tags to the end (overwrite the other values)
        forceParams = options["doxygen_paramsForce"]
        for force in forceParams.keys():
            config += "\n" + force + " = " + forceParams[force]
        
        #Call doxygen, piping the config to it
        with subprocess.Popen([options["doxygen_binaryPath"] + "/doxygen.exe", "-"], stdin=subprocess.PIPE, universal_newlines=True) as proc:
            proc.communicate(input=config, timeout=20)

    #( 2 )Sort through all files and get the ones we want to parse
    #List of all the actual wiki pages
    wikiPages = []
    
    for root, dirs, files in os.walk("./tmp/html"):
        for file in files:
            #Get all the file info
            fileAbsPath = os.path.abspath(root + "\\" + file)
            fileRelPath = os.path.relpath(fileAbsPath, "./tmp/html")
            filePath, fileTail = os.path.split(fileRelPath)
            fileName, fileExt = os.path.splitext(fileTail)
            
            #Filter out by extension
            if fileExt != ".html":
                print(".", end="", flush=True)
                continue
       
            #Check special files and type
            fileDoxyType = None
            #Special ("other") files
            for other in options["doxygen_otherFiles"]:
                if fileName == other:
                    fileDoxyType = "OTHER"
            
            #Check type
            if not fileDoxyType:
                for prefix in options["doxygen_filePrefixes"]:
                    if fileName[:len(prefix)] == prefix:
                        fileDoxyType = prefix
            
            #Filter out the html files without type
            if fileDoxyType == None:
                print(".", end="", flush=True)
                continue
            
            print("*", end="", flush=True)
            
            #Make the doxygen wiki page object
            page = DoxygenWikiPage()
            page.filepath = fileAbsPath
            page.filename = fileTail
            page.type = fileDoxyType
            
            wikiPages.append(page)
    print("")
    
    #( 3 )Extract the wiki data from each file
    for page in wikiPages:
        print("Parsing " + page.filename)
        fp = open(page.filepath)
    
        #Extract the specific parts of the page for the wiki
        extractor = DoxygenHTMLExtractor()
        extractor.feed(fp.read())
        
        if not "title" in extractor.data or not "contents" in extractor.data:
            print("Not enough information from Doxygen file for wiki page")
            continue
        
        page.title = extractor.data["title"]
        page.contents = extractor.data["contents"]
    
    #( 4 )Ready the page by getting everything into valid wiki markup
    for page in wikiPages:
        print("Converting " + page.filename)
        
        #Translate gathered data into MediaWiki markup
        #Currently we're only worried about "body > div.contents" having translatable markup in it
        translator = DoxygenHTMLReplacer(wikiPages) #Send it wikiPages so it can translate doxygen links to mediaWiki links
        translator.feed(page.contents)
        
        page.contents = translator.data
        page.imgs = translator.imgs
        
        #Add categories to the pages
        page.contents = "[[Category:" + options["mediaWiki_separationName"] + "_" + page.type + "]]"
    
    #( 5 )Create necessary pages on the wiki
    #Make sure we're logged in - Use login.py script
    """login.main(["-pass:not_the_real_password_replace_this"])
    
    #First delete everything in the autogenerated category
    gen = pagegenerators.CategorizedPageGenerator(options["mediaWiki_separationName"], recurse=True)
    gen = pagegenerators.PreloadingPageGenerator(gen)
    for page in gen:
        if page.isCategory:
            continue
        else:
            page.get()
            text = page.text
            if page.text.find("Doxygen object no longer exists"):
                page.delete()
            else:
                try:
                    page.text = "Doxygen object no longer exists"
                    # Save the page
                    page.save()
                except pywikibot.LockedPage:
                    
                except pywikibot.EditConflict:
                    
                except pywikibot.SpamfilterError as error:
                    
                else:
                
    
    #Now create all the pages
    for pageData in wikiPages:
        gen = pagegenerators.PagesFromTitlesGenerator([pageData.title])
        #TODO: We should eventually use a PreloadingPageGenerator (send out bursts of requests?)
        #  and not make a generator one page at a time
        
        for page in gen:
            #Whether the page exists or not just create/overwrite it
            try:
                page.text = text
                # Save the page
                page.save()
            except pywikibot.LockedPage:
                
            except pywikibot.EditConflict:
                
            except pywikibot.SpamfilterError as error:
                
            else:
                
                
        
        
        #Create/overwrite category (add it to main category)
        #TODO: Check ifExists first
        gen = pagegenerators.PagesFromTitlesGenerator(["Category:" + options["mediaWiki_separationName"] + "_" + pageData.type])
        for page in gen:
            try:
                page.text = "[[Category:" + options["mediaWiki_separationName"] + "]]"
                page.save()
            except pywikibot.LockedPage:
                
            except pywikibot.EditConflict:
                
            except pywikibot.SpamfilterError as error:
                
            else:
                
        #Create the transcription pages
        if options["mediaWiki_setupTranscripts"]:
            prefix = ""
            if "mediaWiki_transcriptionPrefix" in options and options["mediaWiki_transcriptionPrefix"] != "":
                prefix = options["mediaWiki_transcriptionPrefix"] + " "
            gen = pagegenerators.PagesFromTitlesGenerator([prefix + pageData.title])
            
            category = ""
            if "mediaWiki_transcriptionCategory" in options and options["mediaWiki_transcriptionCategory"] != "":
                category = "[[Category:" + options["mediaWiki_transcriptionCategory"] + "]]"
            
            for page in gen:
                if not page.exists() or page.isRedirectPage():
                    #If page does not exist, create it as a redirect to the original documentation page
                    page.text = "#REDIRECT [[" + options["mediaWiki_separationName"] + "_" + pageData.title + "]]\n" + category
                else:
                    #First make sure the category is on the page
                    hasCat = False
                    if category != "":
                        for cat in page.categories():
                            if cat.title == category:
                                hasCat = True
                                break
                    else
                        hasCat = True
                    
                    if hasCat:
                        #If the page does exist and the template IS on there, do nothing
                        #now check to see if it includes the template
                        page.get()
                        text = page.text
                        pos = text.find("{{" + options["mediaWiki_separationName"] + "_" + pageData.title + "}}")
                        if pos == -1:
                            #append it
                            try:
                                page.text = page.text + "{{" + options["mediaWiki_separationName"] + "_" + pageData.title + "}}"
                                page.save()
                            except pywikibot.LockedPage:
                                
                            except pywikibot.EditConflict:
                                
                            except pywikibot.SpamfilterError as error:
                                
                            else:
                

        #Upload all images - Use upload.py script
        for img in page.imgs:
            upload.main(["-keep", img, "Autogenerated Doxygen Image"])"""
        
    #( 6 ) We're done!
    print("done")

if __name__ == '__main__':
    main()